{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07IC2P1tQz9M"
   },
   "source": [
    "# COMP5328 - Advanced Machine Learning\n",
    "\n",
    "## Tutorial - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ss-mhQXWQz9N"
   },
   "source": [
    "**Semester 2, 2025**\n",
    "\n",
    "**Objectives:**\n",
    "* To familiar with a popular reinforcement learning environment openAI gym.\n",
    "* To implement the deep Q-learning with memory replay with Pytorch.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"week11_tutorial.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"week11_tutorial.ipynb\" file\n",
    "* Complete exercises in \"week11_tutorial.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "Lecturers: Tongliang Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87YKIL5iQz-P"
   },
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you’ll need to have Python 3.5+ installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install pytorch via pip3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To install pytorch with a specifical version in command line:**\n",
    "\n",
    "pip3 install torch==\\\\$TORCH_VERSION_NUMBER torchvision==\\$TORCHVISION_VERSION_NUMBER \\\\$SOURCE_URL\n",
    "\n",
    "To work properly, different systems and GPUs require different versions of pytorch. Please check out https://pytorch.org/get-started/locally/ carefully before installation. You can also find instructions for other installation methods. \n",
    "\n",
    "You can validate whether you have installed correctly via following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install gym via pip3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To install gym in command line:** \n",
    "    \n",
    "pip3 install gym\n",
    "\n",
    "Similarly, you can validate whether you have installed correctly via following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to openAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A Minimum Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have installed gym correctly, by runing following code, you should observe a pop-up window that a cart-Pole starts at the middle position and does random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyao0814/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(250):\n",
    "    env.render()\n",
    "    #take a random action from action space\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain detailed documentation via their offical website (https://gym.openai.com/docs/). Here, we summarise some key points.\n",
    "\n",
    "**observation or state (object)**: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "**reward (float)**: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "**done (boolean)**: whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "**info (dict)**: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 A Best Code Practice in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide a highly readable and scalable code structure for training RL aloghrihms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 31 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 24 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 11 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class DummpyAgent():\n",
    "    \"\"\"For best practice, we define an Agent object with three methods that interacts with environment\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    # This method is used to update network parameters, store experiences into memory buffer (if any) ...\n",
    "    def step(self):\n",
    "        self.update_parameters()\n",
    "        \n",
    "    # This method is used to sample actions.\n",
    "    # This dummpy agent takes actions radnomly without looking at observation and rewards.  \n",
    "    def act(self):\n",
    "        action = self.env.action_space.sample()      \n",
    "        return action\n",
    "    \n",
    "    # For the best practice, to reduce the code length of the step method, we split update rules out from the step method.\n",
    "    # This dummpy agent does not have any parameter to update.\n",
    "    def update_parameters(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DummpyAgent(env)\n",
    "\n",
    "for i_episode in range(20):\n",
    "    state = env.reset()\n",
    "    # The CartPole environment totally has 50 time steps, i.e., the environment will be reset every 50 time step.\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = agent.act()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        agent.step()\n",
    "        # Done indicates whether the game is finished or not.\n",
    "        # In this environment, done equals true when the cart-pole is off-screen (game over), then we should end this episode.\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQN with Memory Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first provide the pseudocode code of the QDN with Memory Replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![texte](https://blog.oliverxu.cn/2019/12/01/Playing-Cartpole-with-natural-deep-reinforcement-learning/dqn_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Replay Buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-a40bc9e61aac>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a40bc9e61aac>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    def sample(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"This object is used to store experiences.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        self.device = device\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        ##TODO Add a new experience to the memory buffer.\n",
    "        \n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory buffer\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"override the len method\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define a Q-network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create a two layer neural network to model Q-value function\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN,self).__init__() \n",
    "        self.fc1= nn.Linear(state_size,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.out = nn.Linear(128, action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #TODO ensembling the defined layers together with relu activation function\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define a Smart Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a general agent that models Q-value function with the neural network, and learns a batch of experience from the memory buffer. This agent could play any game with discrete actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
    "    def __init__(self, env, replay_buffer_size = int(100000), batch_size = 64, lr = 5e-4, gamma=0.99, update_interval = 5):\n",
    "        self.update_step = 0\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        # init a network used to model Q-value function\n",
    "        self.qnetwork = DQN(state_size = self.state_size, action_size = self.action_size).to(self.device)\n",
    "\n",
    "        # optimization method\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(),lr=lr)\n",
    "\n",
    "        # init replay memory \n",
    "        self.memory = ReplayBuffer(\n",
    "            action_size = self.action_size, \n",
    "            batch_size = self.batch_size,\n",
    "            buffer_size= self.replay_buffer_size, \n",
    "            device = self.device\n",
    "            )\n",
    "\n",
    "\n",
    "    def step(self, state, action, reward, next_step, done):\n",
    "        ## TODO save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "        # Updating the network parameters after every update_interval time steps.\n",
    "        self.update_step = (self.update_step+1)% self.update_interval\n",
    "        if self.update_step == 0:\n",
    "            # If enough samples are available in memory, random sample batch of experiences from memory and learn\n",
    "            if len(self.memory)>self.batch_size:\n",
    "                ## TODO sample experiences and update network parameters\n",
    "           \n",
    "\n",
    "    def act(self, state, exploration_rate):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork(state)\n",
    "        ## TODO \n",
    "        # 1) With probability 1-exploration_rate, this agent follows the action that maximizes the estimated Q values.\n",
    "        # 2) With probability exploration_rate, this agent randomly sample actions from action space for exploration.\n",
    "\n",
    "     \n",
    "    def update_parameters(self, experiences):\n",
    "        \"\"\"update network parameters\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        self.qnetwork.train()\n",
    "        ## TODO calcuate the estimated q_value based on current the current state\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_value_next = self.qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        q_value = rewards + (self.gamma * q_value_next*(1-dones))\n",
    "        loss = criterion(q_value_old, q_value).to(self.device)\n",
    "        ## TODO update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the agent with CartPole environment (https://gym.openai.com/envs/CartPole-v0/). The CartPole will be balanced after 400th episode apporixmately, and the maximium score is 50.\n",
    "\n",
    "**To accelerate the training process, you could comment \"env.render()\" out. Then the code will be run without rendering the animation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c5X7is4HQz-P"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "agent = Agent(env)\n",
    "exploration_rate = 1.0\n",
    "for i_episode in range(450):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    # The CartPole environment totally has 50 time steps, i.e., the environment will be reset every 50 time step.\n",
    "    for t in range(50):\n",
    "        # You can comment the following line to accelerate the training process.\n",
    "        env.render()\n",
    "        \n",
    "        action = agent.act(state,exploration_rate)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.step(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        exploration_rate = max(exploration_rate*0.996,0.01)\n",
    "    print('Score for '+str(i_episode)+\" is \"+str(score))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2-ufl-with-answer.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
