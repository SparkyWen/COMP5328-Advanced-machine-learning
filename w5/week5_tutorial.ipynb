{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5328 - Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial - Approximation Error and Estimation Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2, 2025**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To understand the concept of approximation error and estimation error.\n",
    "* To show the approximation error and estimation error in practical examples.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"week4_tutorial.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"week4_tutorial.ipynb\" file\n",
    "* Complete exercises in \"week4_tutorial.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "Lecturers: Tongliang Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Approximation Error and Estimation Error\n",
    "* Let $X$ and $Y$ be variables of features and labels (real values or numerical categorical values), respectively. We also denote $S=\\{(X_1,Y_1),\\cdots,(X_n,Y_n)\\}$ as a training sample with sample size $n$. This training sample is assumed to drawn independently according to the distribution $P(X,Y)$.\n",
    "\n",
    "\n",
    "* Let $h$ be a hypothesis function which tries to fit the data. For example, in classification and regression, a hypothesis function is a mapping from input features to labels; that is, $h:X\\mapsto Y$. We denote $\\mathcal{H}$ as a predefined Hypothesis class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loss Function and Risk\n",
    "* To measure the performance of each hypothesis function on data fitting, loss function is introduced. For example, in classification, loss functions such as 0-1 loss measure the error between the true labels and predicted labels. We denote the loss function as $\\ell(X,Y,h)$.\n",
    "\n",
    "\n",
    "* Given a loss function and a hypothesis function, the $\\textbf{expected risk}$ is defined as $R(h)=\\mathbb{E}\\left[\\ell(X,Y,h)\\right]=\\int_{X,Y} P(X,Y)\\ell(X,Y,h)dXdY$.\n",
    "\n",
    "\n",
    "* In fact, we only have access to training examples drawn according to the distribution $P(X,Y)$. Thus, we approximate the expected risk using the $\\textbf{empirical risk}$; that is, $R_S(h) = \\sum_{i=1}^n \\ell(X_i,Y_i,h)$.\n",
    "\n",
    "\n",
    "* Viewing $\\{(X_1,Y_1),\\cdots,(X_n,Y_n)\\}$ as independently and identically distributed random variables, then $R(h)=\\mathbb{E}\\left[R_S(h)\\right]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Target Concept, the Best Hypothesis in the Hypothesis Class, and the Learned Hypothesis\n",
    "* The target concept $c$ is the best hypothesis function in the universal function space; that is, $c=\\arg\\min_{h} R(h)$.\n",
    "\n",
    "\n",
    "* The best hypothesis function $h^*$ in the hypothesis class is $h^*=\\arg\\min_{h\\in \\mathcal{H}} R(h)$.\n",
    "\n",
    "\n",
    "* The learned hypothesis function $h_S$ is obtained by minimizing the empiricial risk; that is, $h_S = \\arg\\min_{h\\in \\mathcal{H}} R_S(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Approximation Error and Estimation Error\n",
    "* The $\\textbf{approximation error}$ is caused by difference between $h^*$ and $c$. Here, we use the difference $R(h^*)-R(c)$ to measure the approximation error.\n",
    "\n",
    "\n",
    "* The $\\textbf{estimation error}$ is caused by $h_S$ and $h^*$. Here, we use the difference $R(h_S)-R(h^*)$ to measure the estimation error.\n",
    "\n",
    "\n",
    "* We can easily see that the difference $R(h_S) - R(c)$ can be decompose to $R(h_S)-R(h^*)+R(h^*)-R(c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Case 1: Target Concept is in the Hypothesis Class\n",
    "* We first consider the case in which the target concept is included in the predefined hypothesis class $\\mathcal{H}$. In this case, $h^*=c$ and the approximation error will be 0.\n",
    "\n",
    "\n",
    "\n",
    "**Example:**\n",
    "* Here, we use linear logistic regression as an example to illustrate how the estimation error changes when the number of training examples is increasing. \n",
    "\n",
    "\n",
    "* Recall that, in logistic regression, we let $h(x,w) = \\log \\frac{p(y=1|x,w)}{p(y=-1|x,w)}$. If $p(y=1|x,w)>p(y=-1|x,w)$, $h(x,w)>0$ and $x$ should be classified to $1$; otherwise, $-1$. Here, we define $h(x,w)=0$ as a decision boundary to split the data into two classes. More details about decision boundary can be found at https://en.wikipedia.org/wiki/Decision_boundary.\n",
    "\n",
    "\n",
    "* In this example, we assume all training data are two dimensional vectors, i.e., $X \\in \\mathbb{R}^2$. The true decision boundary is set to be $c(X) = X^{\\top} \\left[\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right]=0$. If $X^{\\top} \\left[\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right]\\geq 0$, the data is labelled as $1$; otherwise, $-1$. \n",
    "\n",
    "\n",
    "* We also assume the predefined hypothesis class is linear; that is, $h(X,w) = X^{\\top} w$. We can easily see, the target concept $c(X) = X^{\\top} \\left[\\begin{array}{c} 1 \\\\ -1 \\end{array} \\right]$ is included in the predefined hypothesis class.\n",
    "\n",
    "\n",
    "\n",
    "* The visualization of the training data is shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Visualization of the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "nsample = 20000\n",
    "X = -5 + 10*np.random.rand(2,nsample)\n",
    "Y = np.zeros(nsample)\n",
    "for i in range(nsample):\n",
    "    if X[0][i]-X[1][i]>=0:\n",
    "        Y[i] = 1\n",
    "    else:\n",
    "        Y[i] = -1\n",
    "\n",
    "# visualization of the training data\n",
    "plt.scatter(X[0],X[1],c=['b' if X[0][i]-X[1][i]<0.0 else 'r' for i in range(nsample)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 0-1 Loss and Empirical Risk\n",
    "* According to tutorial 2, we use the function $\\textrm{sign}\\left(\\frac{1}{1+\\exp(-X^{\\top}w)}-\\frac{1}{2}\\right)$ as a classifier. Here, $\\textrm{sign}(Z)=1$ if $Z\\geq 1$; otherwise, $\\textrm{sign}(Z)=-1$. This is equivalent to $\\textrm{sign}\\left(X^{\\top}w\\right)$ (Please check by yourself).\n",
    "\n",
    "\n",
    "* Note: For $\\textrm{sign}(Z)$, the particular choice of $\\textrm{sign}(0)$ is not important.\n",
    "\n",
    "\n",
    "* The 0-1 loss function is defined as $1\\left(Y\\neq \\textrm{sign}\\left(X^{\\top}w\\right)\\right)$ where $1(Z)=1$ if $Z$ is true; otherwise, $1(Z)=0$.\n",
    "\n",
    "** Exercise 1.4.1 **\n",
    "* Given the estimated parameters, compute the empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empiricalRisk(X,Y,w):\n",
    "    \"\"\"Compute the empirical risk of hypothesis function with parameter w.\n",
    "    Input: \n",
    "       X: array, shape = [d, n]\n",
    "          The features. Here, d is the dimension of features, n is the sample size.\n",
    "       Y: array, shape = [n]\n",
    "          The labels.\n",
    "       w: array, shape = [d]\n",
    "          The parameter of the hypothesis function.\n",
    "    Output:\n",
    "       risk: real value\n",
    "          The empirical risk of this hypothesis function with parameter w.\n",
    "    \"\"\"\n",
    "    TODO\n",
    "\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Linear Logistic Regression\n",
    "** Exercise 1.4.2 **\n",
    "* Given the training data, estimate the parameters of hypothesis function by using gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearLR(X,Y,lr,maxiter,tol):\n",
    "    \"\"\"Estimate parameters of hypothesis function.\n",
    "    Input: \n",
    "       X: array, shape = [d, n]\n",
    "          The features. Here, d is the dimension of features, n is the sample size.\n",
    "       Y: array, shape = [n]\n",
    "          The labels.\n",
    "       lr: real value\n",
    "          The learning rate.\n",
    "       maxiter: int\n",
    "          The maximum number of iterations.\n",
    "       tol: real value.\n",
    "          The tolerance used for stopping the iterative process.\n",
    "    Output:\n",
    "       w: array, shape = [d]\n",
    "          The estimated parameters of hypothesis function.\n",
    "    \"\"\"\n",
    "    \n",
    "    TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Approximation Error and Estimation Error\n",
    "* In this example, as mentioned above, the target concept is included in the predefined hypothesis function. Then the approximation error is 0. We mainly study the estimation error which is caused by the difference between $h_S$ and $h^*$.\n",
    "\n",
    "\n",
    "* Further, it is easy to see the expected risk $R(c)=R(h^*)=0$ because the target concept can correctly classify all data. Then, $R(h_S) - R(h^*) = R(h_S)$ which measures the estimation error. We can thus study whether the estimation error converges to 0 when the sample size increases to $+\\infty$ by using the term $R(h_S)$.\n",
    "\n",
    "\n",
    "\n",
    "* However, we have no knowledge of distribution $P(X,Y)$. Thus, in this example, we sample a large amount of data and use the empirical risk to approximate $R(h_S)$.\n",
    "\n",
    "\n",
    "** Exercise 1.4.3 **\n",
    "* Given training data with increasing sample sizes, study the convergence of the estimation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we begin the estimation of sample size: 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58b1c25cf707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# estimate w and estimation error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mTODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrisk_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_estimation_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "# sample size\n",
    "num = [10, 50, 100, 500, 1000, 5000, 10000]\n",
    "risk_list = []\n",
    "for n in num:\n",
    "    print('Now we begin the estimation of sample size: '+str(n))\n",
    "    arr = np.arange(nsample)\n",
    "    np.random.shuffle(arr)\n",
    "    arr = arr[:n]\n",
    "    new_X = X[:,arr]\n",
    "    new_Y = Y[arr]\n",
    "    \n",
    "    # estimate w and estimation error\n",
    "    TODO\n",
    "    \n",
    "    risk_list.append(approx_estimation_error)\n",
    "\n",
    "plt.plot(np.log10(num), risk_list, 'r--', np.log10(num), [0]*len(num),'b--')\n",
    "plt.xlabel('Log10 of Sample Size')\n",
    "plt.ylabel('Approximated Estimation Error')\n",
    "plt.show()\n",
    "print(risk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Case 2: Target Concept is Not in the Hypothesis Class\n",
    "* Second, we consider the case in which the target concept is not included in the predefined hypothesis class $\\mathcal{H}$. In this case, $h^*\\neq c$ and the approximation error will not be 0.\n",
    "\n",
    "** Example: **\n",
    "* Here, we use linear regression as an example.\n",
    "\n",
    "\n",
    "* We set the target concept as $y = x^2$ where $x \\in [0,1]$.\n",
    "\n",
    "\n",
    "* We assume the hypothesis function is linear; that is $h(x,\\beta_0,\\beta_1) = \\beta_0 x+\\beta_1$. Then, it is easy to see that the target concept is not included in the predefined hypothesis class. Therefore the approximation error is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Visualization of the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "nsample = 20000\n",
    "X = np.random.rand(nsample)\n",
    "Y = X*X\n",
    "print(X)\n",
    "\n",
    "# visualization of the training data\n",
    "plt.scatter(X,Y,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Square Loss and Empirical Risk\n",
    "* In linear regression, we exploit square loss to evaluate a hypothesis function; that is $\\ell(X,Y,h) = (Y-h(X))^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.5.1**\n",
    "* Given the training data and estimated paramters, compute the empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def empiricalRiskSquareLoss(X,Y,beta_0,beta_1):\n",
    "    \"\"\"Compute the empirical risk of hypothesis function with parameter w.\n",
    "    Input: \n",
    "       X: array, shape = [n]\n",
    "          The features. The feature is one dimensional.\n",
    "       Y: array, shape = [n]\n",
    "          The labels.\n",
    "       beta_0, beta_1: real values\n",
    "          The parameter of the hypothesis function.\n",
    "    Output:\n",
    "       risk: real value\n",
    "          The empirical risk of this hypothesis function with parameter w.\n",
    "    \"\"\"\n",
    "    TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Linear Regression\n",
    "** Exercise 1.5.2 **\n",
    "* Given the training data, estimate the parameters of hypothesis function by using least square method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegression(X,Y):\n",
    "    \"\"\"Estimate parameters of hypothesis function.\n",
    "    Input: \n",
    "       X: array, shape = [n]\n",
    "          The features. The feature is one dimensional.\n",
    "       Y: array, shape = [n]\n",
    "          The labels.\n",
    "    Output:\n",
    "       beta_0, beta_1: real values\n",
    "          The estimated parameters of hypothesis function.\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4 The Approximation Error and Estimation Error\n",
    "* In this example, we are able to compute the approximation error. Recall that the approximation error is measured by $R(h^*) - R(c)=R(h^*)$ because the target concept outputs the exact label for each feature and the corresponding expected risk $R(c)=0$.\n",
    "\n",
    "\n",
    "* We can minimze $R(h) = \\int_{0}^1 \\frac{1}{1-0} (x^2 - \\beta_0 x - \\beta_1)^2 dx = \\int_{0}^1 (x^2 - \\beta_0 x - \\beta_1)^2 dx$ to achieve the best hypothesis function $h^*$ in the linear hypothesis class. Then we can compute the approximation error $R(h^*)$.\n",
    "\n",
    "\n",
    "* Note that the data are assumed to be uniformly sampled from $[0,1]$ and the probability density function of $X$ is $p(X)=1$ if $0\\leq X\\leq 1$; otherwise $p(X)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1.5.3**\n",
    "* Compute the parameters of the best hypothesis function $h^*$ and the approximation error.\n",
    "\n",
    "\n",
    "* Hint: (1) The form of $R(h)$ can be first computed. We can obtain $R(h)=\\frac{1}{5}-\\frac{\\beta_0}{2}-\\frac{2\\beta_1}{3} + \\beta_0 \\beta_1 + \\frac{\\beta_0^2}{3} + \\beta_1^2$. The computation of integral can be found at https://en.wikipedia.org/wiki/Integral.\n",
    "\n",
    "* (2) Let the gradients of $R(h)$ with respect to $\\beta_0$ and $\\beta_1$ to be 0. We can find the parameters of the best hypothesis function. \n",
    "\n",
    "\n",
    "* The results are: $\\beta_{0}^*=1$, $\\beta_{1}^*=-\\frac{1}{6}$, and $R(h^*) = \\frac{1}{180}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1.5.4**\n",
    "* Increase the number of training examples from 10 to 1000 with step 10. Then show the relationship between the difference $R(h_S)-R(c)$ and sample size.\n",
    "\n",
    "\n",
    "* Note that $R(c) = 0$ because the target concept outputs the exact label for each feature.\n",
    "\n",
    "\n",
    "* Note that we use empirical risk to approximate $R(h_S)$ as in the example of linear logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = range(10,1000,10)\n",
    "risk_list = []\n",
    "\n",
    "for n in num:\n",
    "    arr = np.arange(nsample)\n",
    "    np.random.shuffle(arr)\n",
    "    arr = arr[:n]\n",
    "    new_X = X[arr]\n",
    "    new_Y = Y[arr]\n",
    "    \n",
    "    # estimate beta_0, beta_1 and empirical risk\n",
    "    TODO\n",
    "    risk_list.append(empirical_risk)\n",
    "    \n",
    "plt.plot(num,risk_list,'r--', num, [1.0/180]*len(num), 'b--')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Remark **\n",
    "* We can see the empirical risk converges to $R(h^*)$ but will never approach to 0 because the target concept is not included in the predefined hypothesis function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
