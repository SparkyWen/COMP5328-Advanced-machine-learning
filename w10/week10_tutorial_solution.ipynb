{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07IC2P1tQz9M"
   },
   "source": [
    "# COMP5328 - Advanced Machine Learning\n",
    "\n",
    "## Tutorial - Learning with Noisy Data II: Label Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ss-mhQXWQz9N"
   },
   "source": [
    "**Semester 2, 2025**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To build a neural network classifier with Pytorch.\n",
    "* To estimate the transistion matrix.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Exercises to be completed on IPython notebook such as: \n",
    "   * Ipython 3 (Jupyter) notebook installed on your computer http://jupyter.org/install (you need to have Python installed first https://docs.python.org/3/using/index.html )\n",
    "   * Web-based Ipython notebooks such as Google Colaboratory https://colab.research.google.com/ \n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"week10_tutorial.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"week10_tutorial.ipynb\" file\n",
    "* Complete exercises in \"week10_tutorial.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "Lecturers: Tongliang Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we illustrate how to solve the instance-independent and class-dependent transition matrix in a binary classification setting, which can be easily extended to the multi-class classification.\n",
    "\n",
    "Let $Y$ be the clean label, and $\\tilde{Y}$ be the noisy label. Remind that the noisy class posteriors ${P}(\\tilde{Y}|X)$ and the clean class posteriors ${P}(Y|X)$ can be related via the transition matrix $T$, i.e.,\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|X) \\\\\n",
    "    P(\\tilde{Y}=1|X) \n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|Y=0) &  P(\\tilde{Y}=0|Y=1) \\\\\n",
    "    P(\\tilde{Y}=1|Y=0) &  P(\\tilde{Y}=1|Y=1) \n",
    "    \\end{bmatrix} \\begin{bmatrix}\n",
    "    P(Y=0|X) \\\\\n",
    "    P(Y=1|X) \n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Suppose we have a point $x^0$, such that $P(Y=0|X=x^0)=1$ and $P(Y=1|X=x^0)=0$, then we can obtain the first column of the transition matrix as follows,\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|X=x^1) \\\\\n",
    "    P(\\tilde{Y}=1|X=x^1) \n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|Y=0) &  P(\\tilde{Y}=0|Y=1) \\\\\n",
    "    P(\\tilde{Y}=1|Y=0) &  P(\\tilde{Y}=1|Y=1) \n",
    "    \\end{bmatrix} \n",
    "    \\begin{bmatrix}\n",
    "    P(Y=0|X=x^1) \\\\\n",
    "    P(Y=1|X=x^1) \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|Y=0) &  P(\\tilde{Y}=0|Y=1) \\\\\n",
    "    P(\\tilde{Y}=1|Y=0) &  P(\\tilde{Y}=1|Y=1) \n",
    "    \\end{bmatrix} \n",
    "    \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\\begin{bmatrix}\n",
    "    P(\\tilde{Y}=0|Y=0) \\\\\n",
    "    P(\\tilde{Y}=1|Y=0) \n",
    "    \\end{bmatrix},\n",
    "\\end{align}\n",
    "where the noisy class posteriors can be estimated directly by training a classifier with the noisy data.\n",
    "\n",
    "Similarly, if we have a point $x^1$, such that $P(Y=0|X=x^1)=0$ and $P(Y=1|X=x^1)=1$, then we can obtain the second column of the transition matrix.\n",
    "\n",
    "As we can see, the points $x^0$ and $x^1$ are the keys to estimate the transition matrix $T$. These points are called the anchor points which can only belong to one clean class, i.e., given $x^i$ is an anchor point of the clean class $i$, then it must satisfy $P(Y=i|X=x^i) = 1$.\n",
    "\n",
    "When the noise rate is upper bounded by a constant, then the anchor points can be found as follows [1],\n",
    "\\begin{align}\n",
    "x^i = argmax_{x\\in \\mathcal{X}} P(\\tilde{Y}=i|X=x).\n",
    "\\end{align}\n",
    "Emprically, we could estimate the anchor points on the noisy dataset and use the estimated anchor points to estimate the transition matrix, which will be illustrated in the rest of this tutorial.\n",
    "\n",
    "[1]. Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning withbounded instance-and label-dependent label noise. In ICML, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87YKIL5iQz-P"
   },
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided at the tutorial's root folder, which is called \"input.data\". It is a CSV format file. Each row of the file represents an example. The first 10 columns represents the attributes and the last column represents the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 add class-conditional random label noise (CCN)  to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture,\n",
    "* class-conditional random label noise (CCN) is independent on the attributes but dependent on the labels, i.e., $P(\\tilde{Y}|Y,X)=P(\\tilde{Y}|Y)$.\n",
    "* The negative filp rate and the positive filp rate may be not symmetric, i.e., $P(\\tilde{Y}=1|Y=0) \\neq P(\\tilde{Y}=0|Y=1)$, where $Y$ is the clean label, $\\tilde{Y}$ is the noisy label.\n",
    "* The filp rates are usually bounded in [0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms\n",
    "\n",
    "\n",
    "# The data is a 2D array.\n",
    "# Each row of the file represents an example. \n",
    "# The first 10 columns represents their attributes. \n",
    "# The last column represents their labels.\n",
    "# The function should return the noisy data which \n",
    "# has the same shape with the input data. The last column represents their noisy labels.\n",
    "def binary_CCN_generator(data, flip_rates=[0.15,0.25]):\n",
    "    r\"\"\"class-conditional random label noise generator.\n",
    "\n",
    "    Args:\n",
    "        flip rates: [P(Y^{~}=1|Y=0),P(Y^{~}=0|Y=1)]\n",
    "        data: An 2D-array of the \"clean\" data, each row repesents an example. \n",
    "        The last column of the data should be labels {0,1}.\n",
    "        \n",
    "    Returns:\n",
    "        new_data:  An 2D-array of the noisy data.\n",
    "    \"\"\"\n",
    "    # Please don't modify the variable \"data\".\n",
    "    new_data = copy.deepcopy(data)\n",
    "    \n",
    "    # TODO Please complete this function.\n",
    "    label_idx = len(new_data[0])-1\n",
    "    for d in new_data:\n",
    "        curr_label = int(d[label_idx])\n",
    "        if random.uniform(0, 1) <= flip_rates[curr_label]:\n",
    "            d[label_idx] = abs(curr_label - 1)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "# You may test your binary_CCN_generator with the following function.\n",
    "def test_CCN_generator(data_path=\"./input.data\", flip_rates=[0.15,0.25]):\n",
    "    \n",
    "    assert all(fr >= 0 and fr < 0.5 for fr in flip_rates)\n",
    "    assert len(flip_rates) == 2\n",
    "    \n",
    "    data = pd.read_csv(data_path, header = None).values.tolist()\n",
    "    noisy_data = binary_CCN_generator(data = data, flip_rates = flip_rates)\n",
    "    positive_len = 0\n",
    "    postive_f = 0\n",
    "    negative_f = 0\n",
    "    negative_len = 0 \n",
    "\n",
    "    for i in range(len(data)):\n",
    "        d = data[i]\n",
    "        n_d = noisy_data[i]\n",
    "        if d[-1] == 1.0:\n",
    "            positive_len += 1\n",
    "        else:\n",
    "            negative_len += 1\n",
    "        if d[-1] != n_d[-1]:\n",
    "            if n_d[-1] == 0:\n",
    "                postive_f += 1\n",
    "            else:\n",
    "                negative_f += 1\n",
    "    \n",
    "    if (data == noisy_data and sum(flip_rates) != 0) or (negative_f/negative_len-flip_rates[0] > 0.01 and postive_f/positive_len-flip_rates[1]> 0.01):\n",
    "        print(\"Test failed!\")\n",
    "    else:\n",
    "        print(\"Test passed!\")\n",
    "    \n",
    "    \n",
    "test_CCN_generator()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 prepare the noisy data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class, it is used as an input of the DataLoader object.\n",
    "class DatasetArray(Dataset):\n",
    "    r\"\"\"This is a child class of the pytorch Dataset object.\"\"\"\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        if labels != None:\n",
    "            self.data_arr = np.asarray(data).astype(np.float32)\n",
    "            self.label_arr = np.asarray(labels).astype(np.long)\n",
    "        else:\n",
    "            tmp_arr = np.asarray(data)\n",
    "            self.data_arr = tmp_arr[:,:-1].astype(np.float32)\n",
    "            self.label_arr = tmp_arr[:,-1].astype(np.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "     \n",
    "        data = self.data_arr[index]\n",
    "        label = self.label_arr[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label)\n",
    "    \n",
    "    \n",
    "# Splitting the data into three parts.\n",
    "def train_val_test_random_split(data, fracs=[0.7,0.1,0.2]):\n",
    "    r\"\"\"Split the data into training, validation and test set.\n",
    "    Args:\n",
    "        fracs: a list of length three\n",
    "    \"\"\"\n",
    "    assert len(fracs) == 3\n",
    "    assert sum(fracs) == 1\n",
    "    assert all(frac > 0 for frac in fracs)\n",
    "    n = len(data)\n",
    "    subset_lens = [int(n*frac) for frac in fracs]\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    data = np.array(data)\n",
    "    new_data = []\n",
    "    start_idx = 0\n",
    "    for subset_len in subset_lens:\n",
    "        end_idx = start_idx + subset_len\n",
    "        cur_idxs = idxs[start_idx:end_idx]\n",
    "        new_data.append(data[cur_idxs,:].tolist())\n",
    "        start_idx = end_idx\n",
    "    return new_data\n",
    "    \n",
    "    \n",
    "# Preparation of the data for training, validation and testing a pytorch network. \n",
    "# Note that the test data is not in use for this lab.\n",
    "def get_loader(batch_size =128, num_workers = 1, train_val_test_split = [0.7,0.1,0.2], data=None):\n",
    "    r\"\"\"This function is used to read the data file and split the data into three subsets, i.e, \n",
    "    train data, validation data and test data. Their corresponding DataLoader objects are returned.\"\"\"\n",
    "    \n",
    "    [train_data, val_data, test_data] = train_val_test_random_split(data, fracs = train_val_test_split)\n",
    "\n",
    "    train_data = DatasetArray(data = train_data)\n",
    "    val_data = DatasetArray(data = val_data)\n",
    "    test_data = DatasetArray(data = test_data)\n",
    "\n",
    "    #The pytorch built-in class DataLoader can help us to shuffle the data, draw mini-batch,\n",
    "    #do transformations, etc. \n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fully connected network class.\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dim=25, output_dim=2):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO based on the class attributes (fields), define a fully connected network with one hidden layer.\n",
    "        # Specifically, it should have this structure:\n",
    "        # input_layer->relu->hidden_layer->relu->output_layer.\n",
    "        out = F.relu(self.input_layer(x))\n",
    "        out = F.relu(self.hidden_layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def Predefined_FCNet(input_dim=10,hidden_dim=5,output_dim=2):\n",
    "    model = FCNet(input_dim=input_dim,hidden_dim=hidden_dim,output_dim=output_dim)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 some helper functions for training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c5X7is4HQz-P"
   },
   "outputs": [],
   "source": [
    "# When all random seeds are fixed, the python runtime environment becomes deterministic.\n",
    "def seed_torch(seed=1029):\n",
    "    r\"\"\"Fix all random seeds for repeating the expriement result.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # If multi-GPUs are used. \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# Embedding labels to one-hot form.\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    r\"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels] \n",
    "\n",
    "\n",
    "# Calcuate the accuracy according to the prediction and the true label.\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    r\"\"\"Computes the precision@k for the specified values of k.\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "# A helper function which is used to record the experiment results.\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "# Load a NN model.\n",
    "def load_model(m_config):\n",
    "    model = globals()[m_config['model_name']](\n",
    "        input_dim=m_config[\"input_dim\"],\n",
    "        hidden_dim=m_config[\"hidden_dim\"],\n",
    "        output_dim=m_config[\"output_dim\"]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qZfr5evrQz-S"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, criterion, train_loader):\n",
    "    \"\"\"Training a pytorch nn_model.\"\"\"\n",
    "    top1_acc_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    # swith model to to train mode\n",
    "    model.train()\n",
    "    for step, (data, targets) in enumerate(train_loader):\n",
    "        # prepare min_batch\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # predict\n",
    "        preds = model(data)\n",
    "\n",
    "        # forward\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        # set all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update all gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        [top1_acc] = accuracy(preds.data, targets.data, topk=(1,))\n",
    "        # record accuary and cross entropy losss\n",
    "        min_batch_size = data.size(0)\n",
    "        top1_acc_meter.update(top1_acc.item(), min_batch_size)\n",
    "        loss_meter.update(loss.item(), min_batch_size)\n",
    "\n",
    "    print(\"Train epoch \",epoch,\" Accuracy \",top1_acc_meter.avg)\n",
    "    \n",
    "    \n",
    "def validate_and_test(epoch, model, criterion, val_loader, is_test=False):\n",
    "    \"\"\"Validation or testing of a nn_model.\"\"\"\n",
    "    top1_acc_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    # swith model to to eval mode\n",
    "    model.eval()\n",
    "    for step, (data, targets) in enumerate(val_loader):\n",
    "        # prepare min_batch\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            preds = model(data)\n",
    "\n",
    "        # forward\n",
    "        loss = criterion(preds, targets)\n",
    "            \n",
    "        # calculate accuracy\n",
    "        [top1_acc] = accuracy(preds.data, targets.data, topk=(1,))\n",
    "  \n",
    "        # record accuary and cross entropy losss\n",
    "        min_batch_size = data.size(0)\n",
    "        top1_acc_meter.update(top1_acc.item(), min_batch_size)\n",
    "        loss_meter.update(loss.item(), min_batch_size)\n",
    "\n",
    "    top1_acc_avg = top1_acc_meter.avg\n",
    "    if(is_test == False):\n",
    "        print(\"Validate epoch \",epoch,\" Accuracy \",top1_acc_avg)\n",
    "    else:\n",
    "        print(\"Test epoch \",epoch,\" Accuracy \",top1_acc_avg)\n",
    "\n",
    "    return top1_acc_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKFU7j6AQz-X"
   },
   "source": [
    "### 4.3 run training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_331761/3274995061.py:11: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.label_arr = tmp_arr[:,-1].astype(np.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs are used!\n",
      "Train epoch  1  Accuracy  54.33836514076783\n",
      "Validate epoch  1  Accuracy  55.244646733162554\n",
      "Train epoch  2  Accuracy  67.06197033819575\n",
      "Validate epoch  2  Accuracy  71.07685941627308\n",
      "Train epoch  3  Accuracy  70.73940252927092\n",
      "Validate epoch  3  Accuracy  71.12340953365604\n",
      "Train epoch  4  Accuracy  70.73275258926043\n",
      "Validate epoch  4  Accuracy  71.22168200859748\n",
      "Train epoch  5  Accuracy  70.79112451976829\n",
      "Validate epoch  5  Accuracy  71.21133753613873\n",
      "Train epoch  6  Accuracy  70.69063609148759\n",
      "Validate epoch  6  Accuracy  71.41305471988329\n",
      "Train epoch  7  Accuracy  70.778563445016\n",
      "Validate epoch  7  Accuracy  71.00444812011088\n",
      "Train epoch  8  Accuracy  70.78299677186038\n",
      "Validate epoch  8  Accuracy  71.37167683636204\n",
      "Train epoch  9  Accuracy  70.75048584979237\n",
      "Validate epoch  9  Accuracy  71.26823212755889\n",
      "Train epoch  10  Accuracy  70.81772437902579\n",
      "Validate epoch  10  Accuracy  71.33547118670249\n"
     ]
    }
   ],
   "source": [
    "# Some global variabiles\n",
    "device = None\n",
    "train_loader = None\n",
    "model = None\n",
    "model_config = OrderedDict([\n",
    "    ('model_name','Predefined_FCNet'),\n",
    "    ('input_dim',10),\n",
    "    ('hidden_dim',2),\n",
    "    ('output_dim',2)\n",
    "])\n",
    "\n",
    "optim_config = OrderedDict([\n",
    "    ('epochs', 10),\n",
    "    ('batch_size', 1280),\n",
    "    ('base_lr', 0.1),\n",
    "    ('weight_decay', 1e-5),\n",
    "    ('momentum', 0.9),\n",
    "    ('lr_decay', 0.1)\n",
    "])\n",
    "\n",
    "\n",
    "run_config = OrderedDict([\n",
    "    ('seed', 1),\n",
    "    ('outdir', 'trained_model'),\n",
    "    ('num_workers', 1),\n",
    "])\n",
    "\n",
    "data_config = OrderedDict([\n",
    "    ('flip_rates', [0.15,0.25]),\n",
    "    ('data_path', \"./input.data\"),\n",
    "    ('train_val_test_split', [0.7,0.1,0.2])\n",
    "]) \n",
    "\n",
    "config = OrderedDict([\n",
    "    ('model_config', model_config),\n",
    "    ('optim_config', optim_config),\n",
    "    ('run_config', run_config),\n",
    "    ('data_config', data_config)\n",
    "])\n",
    "\n",
    "\n",
    "def run_train_val():\n",
    "    global device\n",
    "    global config\n",
    "    global train_loader\n",
    "    global model\n",
    "    best_top1_acc = 0\n",
    "    # check gpu\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # configerations\n",
    "    run_config = config['run_config']\n",
    "    optim_config = config['optim_config']\n",
    "    data_config = config['data_config']\n",
    "    \n",
    "    # set random seed\n",
    "    seed_torch(run_config['seed'])\n",
    "\n",
    "    # create output directory\n",
    "    outdir = run_config['outdir']\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # load data\n",
    "    data = pd.read_csv(data_config[\"data_path\"], header = None).values.tolist()\n",
    "    data = binary_CCN_generator(data = data, flip_rates = data_config[\"flip_rates\"])\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_loader(\n",
    "        batch_size = optim_config['batch_size'], \n",
    "        num_workers = run_config['num_workers'],\n",
    "        train_val_test_split = data_config['train_val_test_split'],\n",
    "        data = data\n",
    "    )\n",
    "    \n",
    "    # model\n",
    "    model = load_model(config['model_config'])\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(torch.cuda.device_count(), \"GPUs are used!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params = model.parameters(),\n",
    "        lr = optim_config['base_lr'], \n",
    "        momentum = optim_config['momentum'],\n",
    "        weight_decay = optim_config['weight_decay']\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, optim_config['epochs'] + 1):\n",
    "\n",
    "        #train\n",
    "        train(epoch, model, optimizer, criterion, train_loader)\n",
    "\n",
    "        #validation\n",
    "        top1_acc_avg = validate_and_test(epoch, model, criterion, val_loader)\n",
    "        \n",
    "        # save the best model so far\n",
    "        if (top1_acc_avg > best_top1_acc):\n",
    "            state = OrderedDict([\n",
    "                ('config', config),\n",
    "                ('state_dict', model.state_dict()),\n",
    "                ('optimizer', optimizer.state_dict()),\n",
    "                ('epoch', epoch),\n",
    "                ('top1-accuracy', top1_acc_avg),\n",
    "            ])\n",
    "            best_model_path = os.path.join(outdir, 'model_best.pth')\n",
    "            torch.save(state, best_model_path)\n",
    "            best_top1_acc = top1_acc_avg\n",
    "            \n",
    "run_train_val()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Evaluate Flip Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative flip rate： 0.15\n",
      "estimated negative flip rate： 0.16661709547042847\n",
      "positive flip rate： 0.25\n",
      "estimated positive flip rate： 0.28584039211273193\n"
     ]
    }
   ],
   "source": [
    "def eval_flip_rates():\n",
    "    r\"\"\"caluate the negative flip rate and the positive flip rate \n",
    "    by using the prediction of a the trained classifier. \n",
    "    Remind that once we know flip rates, we know the transition matrix, \n",
    "    where the matrix is [[1-negative_flip_rate, positive_flip_rate], [negative_flip_rate, 1-positive_flip_rate]]\n",
    "    \n",
    "    Returns:\n",
    "        negative_flip_rate\n",
    "        positive_flip_rate\n",
    "    \"\"\"\n",
    "    global config\n",
    "    global train_loader\n",
    "    global model\n",
    "    # TODO Please complete this function, three global variables defined above should be used. \n",
    "    # Hit: The structure of this function should be:\n",
    "    # load the saved best model -> get the probability predictions of the training set ->\n",
    "    # find the flip rates based on the probability predictions.\n",
    "    model.load_state_dict(torch.load(os.path.join(config['run_config']['outdir'], 'model_best.pth'))[\"state_dict\"])\n",
    "    model.eval()\n",
    "    pos_condition_p = []\n",
    "    for step, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # TODO Please complete this function.\n",
    "        data = data.to(device)\n",
    "        labels = targets.numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data)\n",
    "\n",
    "        one_hot_labels = one_hot_embedding(targets,2).numpy()\n",
    "        probs = F.softmax(outputs, dim=1).cpu().data.numpy()\n",
    "        data = data.cpu().numpy()\n",
    "        pos_condition_p += probs[:,1].tolist() \n",
    "        \n",
    "    pos_condition_p = sorted(pos_condition_p)\n",
    "\n",
    "    negative_flip_rate, positive_flip_rate = pos_condition_p[0], 1-pos_condition_p[-1]\n",
    "    return negative_flip_rate, positive_flip_rate\n",
    "\n",
    "\n",
    "est_neg_flip_rate, est_pos_flip_rate = eval_flip_rates()\n",
    "print('negative flip rate：', config[\"data_config\"][\"flip_rates\"][0])\n",
    "print('estimated negative flip rate：', est_neg_flip_rate)\n",
    "print('positive flip rate：', config[\"data_config\"][\"flip_rates\"][1])\n",
    "print('estimated positive flip rate：', est_pos_flip_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2-ufl-with-answer.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
